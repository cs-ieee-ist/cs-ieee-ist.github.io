<html>

<head>
  <link rel="stylesheet" type="text/css" href="css/template.css">
  <link rel="stylesheet" type="text/css" href="css/ia.css">
  </link>
  <!-- template.css has all default class names -->

</head>

<body>

  <div id="pageTop">
    <center>
      <div id="projectName">IA</div>
    </center>
    <center>
      <div class="">Inteligência Artificial</div>
    </center>
  </div>

  <div id="pageBody">
    <!-- Page Content -->

    <!-- Agentes Inteligentes -->

    <h3 id="ia--agentes-inteigentes" class="topic-title">Agentes Inteligentes</h3>
    <h6 class="topic-book-chapter">Capitulo 2.1, 2.4.2, 2.4.3, 2.4.6</h6>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente.png">
    </div></br>
    Imagem retirada do livro: Artificial Intelligence: A Modern Approach</br></br>

    <h4 class="sub-topic-title">Agente</h4>

    Um agente é qualquer coisa que consegue captar/perceber o ambiente onde se encontra e que actua nesse mesmo ambiente
    através de atuadores.</br>

    O termo percepção refere-se a um input do agente recebido a qualquer instante. Uma sequência de percepções será
    então a história completa de tudo o que agente ja captou/percebeu.</br></br>
    <i>"an agent’s choice of action at any given instant can depend on the entire percept
      sequence observed to date, but not on anything it hasn’t perceived"</i></br></br>
    O comportamente de um agente pode ser representado matemáticamente por uma função que mapeia qualquer percepção
    recebida numa ação.</br></br>

    Exemplo de um aspirador:</br>
    <div class="ia-snippet">
      <b>função</b> AgenteAspirador ([posição, estadoLocal]) <b>devolve</b> acção</br>
      <div class="ia-snippet-scope">
        Se estadoLocal = Sujo então Aspirar</br>
        Senão se posição = A então Direita</br>
        Senão se posição = B então Esquerda

      </div>
    </div>


    </br>

    <h4 class="sub-topic-title">Agente racional</h4>

    Um agente racional deve procurar fazer “o que está certo”, com base nas suas percepções e nas acções que pode tomar.
    No entanto, o que quer fazer “o que está certo”? Bem, podemos considerar as consequências do comportamento do
    agente. Se a sequência de estados que ambiente sofrer, resultantes das ações do agente, for considerada boa então
    podemos dizer que o agente está a ter uma boa performance.</br></br>

    Definição de agente racional:</br>
    "For each possible percept sequence, a rational agent should select an action that is expected to maximize its
    performance measure, given the evidence provided by the percept
    sequence and whatever built-in knowledge the agent has."</br></br>

    Exemplo do aspirador:</br>
    A medida de desempenho do
    agente aspirador pode ser a sujidade
    aspirada, tempo utilizado, electricidade
    consumida, ruído gerado, etc.</br></br>

    <h4 class="sub-topic-title">Agente Autónomo</h4>

    Um agente é autónomo se o seu conhecimento for determinado apenas pela sua experiência, ou seja, este tem a
    capacidade de aprender e adaptar-se.</br></br>

    <h4 class="sub-topic-title">PEAS</h4>

    – Performance (desempenho)</br>
    – Environment (ambiente)</br>
    – Actuators (actuadores)</br>
    – Sensors (sensores)</br></br>

    PEAS: Agente taxista</br>
    • Desempenho</br>
    – Segurança, destino, lucros,
    legalidade, conforto</br>
    • Ambiente</br>
    – Clientes, estradas, trânsito, transeuntes, tempo</br>
    • Actuadores</br>
    – Volante, acelerador, travão, buzina, pisca</br>
    • Sensores</br>
    – GPS, conta km, velocímetro, nível do depósito,
    temperatura do óleo</br>


    <h4 class="sub-topic-title">Tipos de ambientes</h4>

    <b>- Observável vs Parcialmente Observável</b>

    <ul>
      <li>Num ambiente “observável” os sensores do agente dão acesso ao estado
        completo do ambiente em cada instante de tempo, pelo que não é
        necessário manter um estado interno sobre o mundo.</li>
      <li>Ou seja, o agente consegue a cada instante obter informação correcta e
        actualizada do mundo que o rodeia.</li>
      <li>A maioria dos ambientes não são totalmente observáveis tendo em conta
        o aparelho sensorial comum.</li>
      <li>Quanto mais observável é um ambiente mais fácil a criação de agentes
        que nele operem</li>
    </ul>

    <b>- Estático vs Dinâmico</b>

    <ul>
      <li>Um ambiente estático é um ambiente que não é alterado enquanto
        o agente decide que acção vai tomar</li>
      <li>Um ambiente é semi-dinâmico se este permanece inalterado com a passagem do tempo mas a qualidade do desempenho
        do agente é alterada</li>
      <li>Um ambiente dinâmico pergunta constantemente ao agente o que este quer fazer se não decidir então conta como
        decidir fazer nada.</li>
    </ul>

    <b>- Determinístico vs Estocástico</b>

    <ul>
      <li>Num ambiente Determinístico o estado seguinte do ambiente é
        determinado somente em função do estado actual e da acção
        executada pelo agente – não há incerteza para o agente sobre o estado
        do mundo quando o agente executa uma acção</li>
      <li>Se o ambiente é sempre determinístico excepto para as acções de outros agentes, então o ambiente é estratégico
      </li>
      <li>Os ambientes não determinísticos são bastante mais complexos de lidar
        quando da criação dos agentes</li>
    </ul>

    <b>- Episódico vs Sequencial</b>

    <ul>
      <li>Num ambiente Episódico a experiência do agente está dividida em episódios atómicos (em que cada episódio
        consiste em percepção+acção do agente) e a escolha de cada acção em cada episódio depende apenas do próprio
        episódio</li>
    </ul>

    <b>- Discreto vs Contínuo</b>

    <ul>
      <li>Num ambiente Discreto o agente tem um número limitado de percepções e acções distintas que estão claramente
        definidas</li>
    </ul>

    <b>- Single vs Multi-Agente</b>

    <ul>
      <li>Num ambiente Single Agent, tal como o próprio nome diz, só existe um agente nesse ambiente.</li>
    </ul>

    Exemplo</br>
    <table style="margin: auto;border-collapse: collapse">
      <tr>
        <th>Ambientes</th>
        <th>Observavel</th>
        <th>Agentes</th>
        <th>Deterministico</th>
        <th>Episodico</th>
        <th>Estático</th>
        <th>Discreto</th>
      </tr>
      <tr>
        <td>Palavras cruzadas</td>
        <td>Totalmente</td>
        <td>Single</td>
        <td>Determinístico</td>
        <td>Sequencial</td>
        <td>Estático</td>
        <td>Discreto</td>
      </tr>
      <tr>
        <td>Xadrez com relógio</td>
        <td>Totalmente</td>
        <td>Multi</td>
        <td>Determinístico</td>
        <td>Sequencial</td>
        <td>Semi</td>
        <td>Discreto</td>
      </tr>
      <tr>
        <td>Poker</td>
        <td>Parcialmente</td>
        <td>Multi</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Estático</td>
        <td>Discreto</td>
      </tr>
      <tr>
        <td>Gamão</td>
        <td>Totalmente</td>
        <td>Multi</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Estático</td>
        <td>Discreto</td>
      </tr>
      <tr>
        <td>Taxista</td>
        <td>Parcialmente</td>
        <td>Multi</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Dinâmico</td>
        <td>Contínuo</td>
      </tr>
      <tr>
        <td>Diagnóstico Médico</td>
        <td>Parcialmente</td>
        <td>Single</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Dinâmico</td>
        <td>Contínuo</td>
      </tr>
      <tr>
        <td>Análise de imagens</td>
        <td>Totalmente</td>
        <td>Single</td>
        <td>Determinístico</td>
        <td>Episódico</td>
        <td>Semi</td>
        <td>Contínuo</td>
      </tr>
      <tr>
        <td>Part-picking robot</td>
        <td>Parcialmente</td>
        <td>Single</td>
        <td>Estocástico</td>
        <td>Episódico</td>
        <td>Dinâmico</td>
        <td>Contínuo</td>
      </tr>
      <tr>
        <td>Refinery controller</td>
        <td>Parcialmente</td>
        <td>Single</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Dinâmico</td>
        <td>Contínuo</td>
      </tr>
      <tr>
        <td>Tutor interativo de Inglês</td>
        <td>Parcialmente</td>
        <td>Multi</td>
        <td>Estocástico</td>
        <td>Sequencial</td>
        <td>Dinâmico</td>
        <td>Discreto</td>
      </tr>
    </table>

    <h4 class="sub-topic-title">Tipos de agentes</h4>

    <ul>
      <li>Agentes reflexos simples</li>
      <li>Agentes reflexos baseados em modelos</li>
      <li>Agentes baseados em objectivos</li>
      <li>Agentes baseados em utilidade</li>
      <li>Agentes com aprendizagem</li>
    </ul>

    <b>- Agentes reflexos simples</b></br></br>

    Os agentes de reflexos simples são o tipo de agentes mais simples. Estes escolhem uma ação com base na percepção
    atual, ignorando todo o historial de percepções. O exemplo do aspirador apresentado anteriormente é um agente de
    reflexos simples.</br></br>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente_simples.png">
    </div></br>

    <div class="ia-snippet">
      <b>function</b> SIMPLE-REFLEX-AGENT(percept) <b>returns</b> an action</br>
      <div class="ia-snippet-scope">
        persistent: rules, a set of condition–action rules</br>
        state ← INTERPRET-INPUT(percept)</br>
        rule ← RULE-MATCH(state, rules)</br>
        action ← rule.ACTION</br>
        <b>return</b> action</br>
      </div>
    </div>

    <b>- Agentes reflexos simples</b></br></br>

    Os agentes de reflexos baseados em modelos são agentes que mantêm um estado interno que lhes diz como é que o "mundo
    funciona". O estado interno vai depender do histórico de percepções do agente.</br></br>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente_model_based.png">
    </div></br>

    <div class="ia-snippet">
      <b>function</b> MODEL-BASED-REFLEX-AGENT(percept) <b>returns</b> an action
      <div class="ia-snippet-scope">
        <b>persistent</b>: state, the agent’s current conception of the world state
        <div class="ia-snippet-scope">
          model, a description of how the next state depends on current state and action</br>
          rules, a set of condition–action rules</br>
          action, the most recent action, initially none
        </div>
        state ← UPDATE-STATE(state, action, percept, model)</br>
        rule ← RULE-MATCH(state, rules)</br>
        action ← rule.ACTION</br>
        <b>returns</b> action
      </div>
    </div>

    <b>- Agentes baseados em objetivos</b></br></br>

    Conhecer algo o estado atual do ambiente nem sempre é suficiente para decidir o que fazer. Por exemplo, num
    cruzamento, o taxi pode virar à esquerda, virar à esquerda ou ir em frente. A melhor decisão vai depender do local
    para onde o taxi quer ir. Um agente (model based) pode combinar com o modelo um objetivo de forma a escolher a
    melhor ação para alcançar esse mesmo objetivo.</br></br>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente_goal_based.png">
    </div></br>

    <b>- Agentes baseados em utilidade</b></br></br>

    Este agente tem uma função de utilidade. Esta função de utilidade permite estabelecer preferências
    entre sequências de estados que permitem atingir os mesmos objectivos. Por exemplo, um agente taxista que
    pretende chegar a um destino, a função de utilidade permite distinguir as diferentes
    formas de chegar ao destino, em função do tempo, da despesa, da segurança.</br></br>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente_utility_based.png">
    </div></br>

    <b>- Agentes com aprendizagem</b></br></br>

    Este agentec orresponde à ideia de máquina inteligente caracterizada por Turing (1950), em que o agente actua num
    mundo
    inicialmente desconhecido. Este é composto por três elementos:
    <ul>
      <li>O Elemento de aprendizagem, responsável por tornar o agente mais eficiente ao longo do tempo. Através do
        feedback da crítica que avalia actuação do agente de acordo com o desempenho espectável.</li>
      <li>O Elemento de desempenho, responsável por seleccionar as acções do agente</li>
      <li>O Elemento de geração de problemas, responsável por sugerir acções que podem trazer informação útil.</li>
    </ul>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/agente_learning.png">
    </div></br>

    <!-- Fim Agentes Inteligentes -->
    <!-- Aprendizagem por reforco-->

    <h3 id="ia--aprend-reforco" class="topic-title">Aprendizagem por reforço</h3>
    <h6 class="topic-book-chapter">Capitulo 21.1, 21.2 e 21.3</h6>

    Nesta cadeira o tipo de Aprendizagem por reforço que estudamos modela o problema da seguinte forma:</br>

    • Conjunto de estados: X = x1, x2, ..., xn </br>
    • Conjunto de ações: A = a1, a2, ..., am </br>
    • Funcão de reward: R: X × A → |R </br>
    Função V (x) que calcula o reward obtido após uma trajectória com T passos que começa no estado x.
    Se assumirmos γ como o factor de desconto, podemos calcular V (x) da seguinte forma:</br></br>
    <img class="latex-img" src="http://latex.codecogs.com/gif.latex?V(x) = \sum_{t=0}^{T-1}\gamma ^tr^t" /></br></br>
    Matriz Q com dimensões X x A</br></br>
    Funcão Q : X × A → R, calcula o mesmo que V mas para um par estado acção inicial.
    Uma política π(a|x) : X × A → [0, 1] define a probabilidade de se executar cada acção a para cada
    estado x. À política que maximiza o reward total chamamos política óptima π∗
    . Esta pode ser extraída
    da função Q da seguinte forma:</br></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?\pi^\ast(x) = argmax_{a\in A} Q^\ast(x,a)" /></br></br>

    Assim, para podermos aprender a política
    óptima para um dado ambiente, apenas precisamos de aprender a função Q∗
    . Utilizando Q-learning
    podemos apróximar a função Q∗ a partir de várias trajectórias de exemplo no ambiente.
    Se considerarmos um passo de uma trajectória como o conjunto (x, a, y, r), onde:</br></br>

    • x: estado antes de fazer a acção</br>
    • a: acção utilizada</br>
    • y: estado após fazer a acção</br>
    • r: reward obtida por fazer a acção a no estado x</br>
    Podemos assim usar a seguinte regra de update (alpha é o <i>learning rate</i>):</br></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(x,a) = Q(x,a) + \alpha(R(x,a) + max_{b}Q(y,b) - Q(x,a))" /></br></br>

    Exame 2 2018/2019</br></br>

    Considere o seguinte ambiente com 5 estados:</br></br>

    <table style="width:300px;height: auto;margin: auto;border-collapse: collapse">
      <style>
        td {
          border: 1px solid;
          text-align: center;
        }

        tr {
          height: 50px;
        }
      </style>
      <tr>
        <td>1</td>
        <td>2</td>
        <td>3</td>
      </tr>
      <tr>
        <td>4</td>
        <td>5</td>
      </tr>
    </table></br></br>

    Em cada estado podem ser executadas 4 acções, cima (↑), direita (→), baixo (↓) e esquerda (←), sendo que
    aquelas que atravessam uma linha a tracejado fazem o agente transitar entre estados e as restantes fazem com
    que este se mantenha no mesmo estado. O agente recebe uma recompensa de 1 sempre que executa uma acção
    que o faz atravessar uma linha a tracejado, excepto ao executar a acção → no estado 2, caso em que recebe uma
    recompensa de 4. Nos restantes casos o agente tem uma penalização de 2, isto é, recebe uma recompensa de
    -2.</br></br>

    8.1. (1.0) Calcule os updates feitos pelo algoritmo Q-Learning quando o agente começa no estado 1 e executa a
    seguinte sequência de acções: → ↑ → → ←. Assuma que os valores de Q estão inicialmente a 0 e
    considere α = 0.5 e γ = 0.5.</br></br>

    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(1, \rightarrow) = Q(1, \rightarrow) + \alpha(r(1, \rightarrow) + \gamma max_{b}Q(2, b) - Q(1, \rightarrow)) = 0 + 0.5 * (1 + 0.5 * 0 - 0) = 0.5" /></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(2, \uparrow) = Q(2, \uparrow) + \alpha(r(2, \uparrow) + \gamma max_{b}Q(2, b) - Q(2, \uparrow)) = 0 + 0.5 * (-2 + 0.5 * 0 - 0) = -1" /></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(2, \rightarrow) = Q(2, \rightarrow) + \alpha(r(2, \rightarrow) + \gamma max_{b}Q(3, b) - Q(2, \rightarrow)) = 0 + 0.5 * (4 + 0.5 * 0 - 0) = 2" /></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(3, \rightarrow) = Q(3, \rightarrow) + \alpha(r(3, \rightarrow) + \gamma max_{b}Q(3, b) - Q(3, \rightarrow)) = 0 + 0.5 * (-2 + 0.5 * 0 - 0) = -1" /></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?Q(3, \leftarrow) = Q(3, \leftarrow) + \alpha(r(3, \leftarrow) + \gamma max_{b}Q(2, b) - Q(3, \leftarrow)) = 0 + 0.5 * (1 + 0.5 * 2 - 0) = 1" /></br>
    </br>
    8.2. (0.5) Seguindo uma política de exploitation e assumindo o estado de Q obtido após realizar os updates da
    alínea anterior, quais são as 5 primeiras acções efectuadas pelo agente, começando no estado 1? Indique
    não só as acções, mas também o seu resultado, no formato (estado inicial, acção, estado final, recompensa)</br></br>

    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?(1, \rightarrow, 2, 1)\par(2, \rightarrow, 3, 4)\par(3, \leftarrow, 2, 1)\par(2, \rightarrow, 3, 4)\par(3, \leftarrow , 2, 1)" /></br></br>

    8.3 (0.5) Qual é a política óptima para este ambiente? Caso existam múltiplas acções óptimas num determinado
    estado, represente todas as possibilidades.</br></br>

    <table style="width:300px;height: auto;margin: auto;border-collapse: collapse">
      <style>
        td {
          border: 1px solid;
          text-align: center;
        }

        tr {
          height: 50px;
        }
      </style>
      <tr>
        <td>→</td>
        <td>→</td>
        <td>←</td>
      </tr>
      <tr>
        <td>↑ →</td>
        <td>↑</td>
      </tr>
    </table></br></br>

    <!-- Fim Aprendizagem por reforco-->
    <!-- Aprendizagem parametrica-->

    <h3 id="ia--aprend-param" class="topic-title">Aprendizagem paramétrica</h3>
    <h6 class="topic-book-chapter">Capitulo 18.6.4, 18.7</h6>

    <h4 class="sub-topic-title">Perceptrão</h4>

    <div class="graph-img-container">
      <img class="graph-img" src="./media/img/ia/perceptron.png">
    </div></br></br>

    O Perceptrão é um classificador linear que coloca um separador entre pontos positivos e negativos de uma
    classe. Este é composto por três partes: função de input, função de ativação e output.</br></br>

    <b>Função de input:</b></br></br>
    <img class="latex-img"
      src="http://latex.codecogs.com/gif.latex?in = \sum_{i=0}^{n}w_{i}x_{i}\ ou\ ent\tilde{a}o\ w.x" /></br></br>

    <b>Função de ativação:</b></br>
    <ul>
      <li>Se in = 0, ou seja o ponto está em cima do separador: h(in) = 1.</li>
      <li>Se in > 0 , ou seja o ponto está em acima do separador: h(in) = 1.</li>
      <li>Se in < 0 , ou seja o ponto está em abaixo do separador: h(in)=−1.</li> </ul> <b>Output:</b></br></br>

          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?\hat{y} = h(in) = h(\sum_{i=0}^{n}w_{i}a_{i})" /></br></br>

          Agora que vimos como o Perceptrão faz a classificação, falta-nos ver como ele aprende os pesos, que definem
          o separador. Dado um conjunto de treino X com classificações Y. Começa-se por inicializar o vetor w com valores
          aleatórios. De seguida cálcula-se para cada exemplo no conjunto de treino o output do perceptrão. Se este
          valor for igual ao esperado então não se faz nada. Se for diferente então utilizamos a seguinte regra de
          update para atualizarmos os pesos.</br></br>
          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?w = w + \alpha (y_{i}- \hat{y}) x_{i}" /></br></br>

          <b>Exemplo</b></br>
          Considere o conjunto de treino com 8 pontos:</br></br>


          <img class="latex-img" src="http://latex.codecogs.com/gif.latex?X = \begin{bmatrix}
            -2 & -1 & 0 & 1 & 2 & 1 & 0 & 1 \\ 
            0 & 0.5 & -0.1 & -1.5 & 0 & 0.5 & 0.4 & 1.5
            \end{bmatrix}" /></br></br>


          Com as seguintes classificações:</br></br>
          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?Y= [True\ True\ True\ True\ False\ False\ False\ False]" /></br></br>
          Considere os seguintes pontos de teste.</br></br>
          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?\begin{bmatrix}0\\0\end{bmatrix},\begin{bmatrix}-0.5\\ 0.5\end{bmatrix}" /></br></br>

          (a) Fazendo uma ronda de updates com α = 1, w = [1, 1] e w0 = 0, como fica o vector de pesos?</br></br>
          <b>Resolução:</b></br>
          O primeiro passo é acrescentar uma dimensão a X para podermos tratar o termo de bias
          como um peso normal. Assim, ficamos com:</br></br>
          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?X = \begin{bmatrix}-2 & -1 & 0 & 1 & 2 & 1 & 0 & 1 \\ 0 & 0.5 & -0.1 & -1.5 & 0 & 0.5 & 0.4 & 1.5\\1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\end{bmatrix}" /></br></br>

          Deste modo, podemos escrever os pesos num só vector: w = [1, 1, 0].
          Se definirmos True com +1 e False como −1 ficamos com:</br></br>
          <img class="latex-img" src="http://latex.codecogs.com/gif.latex?Y = [1\ 1\ 1\ 1\ -1\ -1\ -1\ -1]" /></br></br>

          Façamos então um ronda de updates</br></br>

          <div class="graph-img-container">
            <img class="graph-img" src="./media/img/ia/perceptron-example.png">
          </div></br></br>


          (b) Qual a classificação que o Perceptrão treinado retorna para os pontos de teste.</br></br>
          <b>Resolução:</b></br></br>
          <img class="latex-img" src="http://latex.codecogs.com/gif.latex?\hat{y}([0, 0, 1]) = h([-5, -2.8, 0] · [0, 0, 1]) = h(0) = 1\\
          \hat{y}([-0.5, 0.5, 1]) = h([-5, -2.8, 0] · [-0.5, 0.5, 1]) = h(1.1) = 1" /></br></br>

          <h4 class="sub-topic-title">Reqressão Logística</h4>

          O modelo de regressão logística funciona de forma muito semelhante ao Perceptrão. Sendo também classificador
          linear que coloca um separador entre pontos positivos e negativos de
          uma classe.
          No caso do Perceptrão a função de ativação retornava 1 ou −1. No caso da regressão logística, função utilizada
          passa a ser a função logística, também conhecida como sigmoid, que retorna um valor no intervalo [0,1].</br></br>
          <img class="latex-img" src="http://latex.codecogs.com/gif.latex?\sigma(x) = \frac{1}{1+e^{-x}}" /></br></br>
          Outra diferença é a letra usada para identifcar os pesos, por convenção na regressão logística usava-mos o w
          agora passamos a usar o β.</br>
          Assim, dado um ponto x o retorno da regressão logística pode ser definido por:</br></br>
          <img class="latex-img"
            src="http://latex.codecogs.com/gif.latex?p(+|x)=\sigma(\beta.x)=\frac{1}{1+e^{-\beta.x}}" /></br></br>
          Tal como para o Perceptrão existem regras de update que podem ser utilizadas para aprender os pesos.
          Contudo, nesta cadeira esses métodos não são abordados.</br></br>


          <!-- Fim Aprendizagem parametrica  -->
          <!-- Aprendizagem nao parametrica  -->

          <h3 id="ia--aprend-n-param" class="topic-title">Aprendizagem não paramétrica</h3>
          <h6 class="topic-book-chapter">Capitulo 18.8</h6>


          <h4 class="sub-topic-title">k-nearest neighbors</h4>
          <p class="topic-content-text">Agora não temos uma fase de treino para calcular os parametros W, como tinhas na
            Aprendizagem Paramétrica. E portanto o peso da computação estará na fase de classificação.
            Dado um exemplo que queremos classificar, começa-se por calcular uma matriz distância que terá a distância
            do
            exemplo a cada ponto e as respetivas labels.
            Para classificar o exemplo faz-se uma média ponderada das labels dos k vizinhos mais próximos.
          </p>

          <img class="latex-img" src="http://latex.codecogs.com/gif.latex?\hat{y}=\frac{\sum_{i=1}^{K} y_{i}}{K}" />

          <p class="topic-content-text">
            <b>Exemplo</b></br>
            Considere o seguinte conjunto de pontos: D = {(−2, 2),(−1, 3),(0, 1),(2, −1)}.</br>
            Calcule a previsão obtida para os pontos (1, y) e (0, y)</br>
            (a) Para K = 2.</br>
            </br>
            O primeiro passo é calcular a matriz de distâncias para ambos os pontos:</br></br>
            <img class="latex-img" src="http://latex.codecogs.com/gif.latex?D = \begin{bmatrix}
      3 & 2 & 1 & 1 \\ 
      2 & 1 & 0 & 2
     \end{bmatrix}" /></br></br>

            <b>(a)</b>Tendo em conta os 2 vizinhos mais próximos, obtemos as seguintes estimativas:</br>
            </br>
            <img class="latex-img"
              src="http://latex.codecogs.com/gif.latex?(0,1),(2,-1) \rightarrow \hat{y}(1) = \frac{1+(-1)}{2} = 0" /></br>

            <img class="latex-img"
              src="http://latex.codecogs.com/gif.latex?(-1,3),(0,1) \rightarrow \hat{y}(0) = \frac{3+1}{2} = 2" /></br>
            </br>
          </p>

          <h4 class="sub-topic-title">Kernels</h4>
          <p class="topic-content-text">
            Um Kernel não é mais do que uma função que se parece com um alto. A função Kernel será usada para pesarmos
            os
            diferentes exemplos no conjunto de treino. Um exemplo que esteja mais longe do ponto a classificar será mais
            penalizado, um
            exemplo mais perto será menos penalizado ou não será penalizado de todo.</br></br>
            Exemplos de diferentes funções que podemos usar como Kernel:<br><br>
            Kernel Quadrático</br></br>

            <img class="latex-img"
              src="http://latex.codecogs.com/gif.latex?K(d) = \max(0,1-left(\left(2\operatorname{abs}\left(d\right)\right)/k)^2\right), k=10" /></br>
            <div class="graph-img-container">
              <img class="graph-img" src="./media/img/ia/kernel_quad.png">
            </div></br></br>

            Kernel Gaussiano</br></br>
            <img class="latex-img"
              src="http://latex.codecogs.com/gif.latex?K(d) = \frac{1}{\sqrt{2\pi}}\times e^{\frac{-d^2}{2}}" /></br></br>

            <div class="graph-img-container">
              <img class="graph-img" src="./media/img/ia/kernel_gauss.png">
            </div></br></br>

            Este tipo de regressão que usa um Kernel segue a mesma lógica do k-nearest neighbors, no entanto, agora
            fazemos
            uma média ponderada sobre todos os elementos do conjunto de treino.</br></br>

            <img class="latex-img"
              src="http://latex.codecogs.com/gif.latex?\hat{y}(x_{q}) = \frac{\sum_{i=1}^{N} y_{i}\ast K(d(x_{q},x_{i}))}{\sum_{i=1}^{N} K(d(x_{q},x_{i}))}" /></br>
          </p>

          <!-- Fim Aprendizagem nao parametrica  -->

          <p class="topic-content-text">
            Exemplos e excertos retirados dos Exercícios de Aprendizagem Automática dos professores: Luís Sá Couto
            (luis.sa.couto@tecnico.ulisboa.pt), Andreas Wichert (andreas.wichert@tecnico.ulisboa.pt), Manuel Lopes
            (manuel.lopes@tecnico.ulisboa.pt)
            <!-- End Page Content -->
  </div>



</body>

</html>